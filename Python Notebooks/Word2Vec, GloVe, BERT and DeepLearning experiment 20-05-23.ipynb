{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "317ae298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18609cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df1 = pd.read_excel('bng2eng2/train/ConscientiousnessTrain.xlsx')\n",
    "df2 = pd.read_excel('bng2eng2/train/AgreeablenessTrain.xlsx')\n",
    "df3 = pd.read_excel('bng2eng2/train/NeuroticismTrain.xlsx')\n",
    "df4 = pd.read_excel('bng2eng2/train/ExtroversionTrain.xlsx')\n",
    "df5 = pd.read_excel('bng2eng2/train/OpennessTrain.xlsx')\n",
    "train_df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
    "train_df = train_df.drop(\"status\", axis='columns')\n",
    "\n",
    "df6 = pd.read_excel('bng2eng2/test/ConscientiousnessTest.xlsx')\n",
    "df7 = pd.read_excel('bng2eng2/test/AgreeablenessTest.xlsx')\n",
    "df8 = pd.read_excel('bng2eng2/test/NeuroticismTest.xlsx')\n",
    "df9 = pd.read_excel('bng2eng2/test/ExtroversionTest.xlsx')\n",
    "df10 = pd.read_excel('bng2eng2/test/OpennessTest.xlsx')\n",
    "test_df = pd.concat([df6, df7, df8, df9, df10], ignore_index=True)\n",
    "test_df = test_df.drop(\"status\", axis='columns')\n",
    "\n",
    "# Data preprocessing\n",
    "# Convert text to lowercase\n",
    "train_df['status_text'] = train_df['status_text'].apply(lambda x: x.lower())\n",
    "test_df['status_text'] = test_df['status_text'].apply(lambda y: str(y).lower())\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "train_df['status_text'] = train_df['status_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "test_df['status_text'] = test_df['status_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "# Tokenization\n",
    "train_df['status_text'] = train_df['status_text'].apply(lambda x: word_tokenize(x))\n",
    "test_df['status_text'] = test_df['status_text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "train_df['status_text'] = train_df['status_text'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "test_df['status_text'] = test_df['status_text'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# Convert list of tokens back to text\n",
    "train_df['status_text'] = train_df['status_text'].apply(lambda x: ' '.join(x))\n",
    "test_df['status_text'] = test_df['status_text'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8091161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using logistic regression and Word2Vec features: 0.23217247097844113\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------------------------------------------\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=train_df['status_text'], vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Function to convert a sentence to its vector representation using Word2Vec\n",
    "def sentence_to_vec(sentence):\n",
    "    word_vectors = [word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv]\n",
    "    if not word_vectors:\n",
    "        # Return zero vector if no words are present in the Word2Vec model\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    return sentence_vector\n",
    "\n",
    "# Convert training and testing data to Word2Vec vectors\n",
    "X_train_word2vec = np.array([sentence_to_vec(sentence) for sentence in train_df['status_text']])\n",
    "X_test_word2vec = np.array([sentence_to_vec(sentence) for sentence in test_df['status_text']])\n",
    "\n",
    "# # Build a Multinomial Naive Bayes model using Word2Vec features\n",
    "# mnb_word2vec_model = MultinomialNB()\n",
    "# mnb_word2vec_model.fit(X_train_word2vec, train_df['label'])\n",
    "\n",
    "# # Evaluate the model on the testing set using Word2Vec features\n",
    "# y_pred_word2vec = mnb_word2vec_model.predict(X_test_word2vec)\n",
    "# accuracy_word2vec = accuracy_score(test_df['label'], y_pred_word2vec)\n",
    "# print(\"Accuracy using Word2Vec features:\", accuracy_word2vec)\n",
    "\n",
    "# Build a logistic regression model using Word2Vec features\n",
    "logreg_word2vec_model = LogisticRegression()\n",
    "logreg_word2vec_model.fit(X_train_word2vec, train_df['label'])\n",
    "\n",
    "# Evaluate the model on the testing set using Word2Vec features\n",
    "y_pred_logreg_word2vec = logreg_word2vec_model.predict(X_test_word2vec)\n",
    "accuracy_logreg_word2vec = accuracy_score(test_df['label'], y_pred_logreg_word2vec)\n",
    "print(\"Accuracy using logistic regression and Word2Vec features:\", accuracy_logreg_word2vec)\n",
    "#-------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cca7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ca86b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using bag-of-words features: 0.3449419568822554\n",
      "Accuracy using TF-IDF features: 0.32172470978441126\n",
      "Accuracy using logistic regression and TF-IDF features: 0.31840796019900497\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction using bag-of-words model\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(train_df['status_text'])\n",
    "X_test_counts = count_vectorizer.transform(test_df['status_text'])\n",
    "\n",
    "# Feature extraction using TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['status_text'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['status_text'])\n",
    "\n",
    "# Build a Multinomial Naive Bayes model using bag-of-words features\n",
    "mnb_counts_model = MultinomialNB()\n",
    "mnb_counts_model.fit(X_train_counts, train_df['label'])\n",
    "\n",
    "# Evaluate the model on the testing set using bag-of-words features\n",
    "y_pred_counts = mnb_counts_model.predict(X_test_counts)\n",
    "\n",
    "accuracy_counts = accuracy_score(test_df['label'], y_pred_counts)\n",
    "print(\"Accuracy using bag-of-words features:\", accuracy_counts)\n",
    "\n",
    "# Build a Multinomial Naive Bayes model using TF-IDF features\n",
    "mnb_tfidf_model = MultinomialNB()\n",
    "mnb_tfidf_model.fit(X_train_tfidf, train_df['label'])\n",
    "\n",
    "# Evaluate the model on the testing set using TF-IDF features\n",
    "y_pred_tfidf = mnb_tfidf_model.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(test_df['label'], y_pred_tfidf)\n",
    "print(\"Accuracy using TF-IDF features:\", accuracy_tfidf)\n",
    "\n",
    "# Build a logistic regression model using TF-IDF features\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train_tfidf, train_df['label'])\n",
    "\n",
    "# Evaluate the model on the testing set using TF-IDF features\n",
    "y_pred_logreg = logreg_model.predict(X_test_tfidf)\n",
    "accuracy_logreg = accuracy_score(test_df['label'], y_pred_logreg)\n",
    "print(\"Accuracy using logistic regression and TF-IDF features:\", accuracy_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8207855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for input text: Neuroticism\n",
      "Accuracy using ensemble of models: 0.32172470978441126\n",
      "Predicted label for input text: Neuroticism\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text input\n",
    "input_text = \"I have a solution for this problem\"\n",
    "input_text = input_text.lower()\n",
    "input_text = ' '.join([word for word in input_text.split() if word not in stop_words])\n",
    "input_text = word_tokenize(input_text)\n",
    "input_text = [stemmer.stem(word) for word in input_text]\n",
    "input_text = ' '.join(input_text)\n",
    "\n",
    "# Extract features from the preprocessed text input\n",
    "X_input = tfidf_vectorizer.transform([input_text])\n",
    "\n",
    "# Predict the label of the input text using the logistic regression model\n",
    "y_pred_input = logreg_model.predict(X_input)[0]\n",
    "\n",
    "# Print the predicted label\n",
    "print(\"Predicted label for input text:\", y_pred_input)\n",
    "\n",
    "# Convert the preprocessed input to Word2Vec vector representation\n",
    "input_vector = sentence_to_vec(input_text)\n",
    "\n",
    "# Reshape the input vector to match the shape expected by the logistic regression model\n",
    "input_vector = input_vector.reshape(1, -1)\n",
    "\n",
    "# Make predictions on the input using the logistic regression model\n",
    "predictions = logreg_word2vec_model.predict(input_vector)\n",
    "\n",
    "logreg_model = LogisticRegression()\n",
    "logreg_model.fit(X_train_tfidf, train_df['label'])\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(X_train_tfidf, train_df['label'])\n",
    "\n",
    "# Ensemble the models\n",
    "ensemble_model = VotingClassifier(estimators=[('mnb_counts', mnb_counts_model), \n",
    "                                               ('mnb_tfidf', mnb_tfidf_model), \n",
    "                                               ('logreg_tfidf', logreg_model),\n",
    "                                               ('logreg_w2v', logreg_word2vec_model),  \n",
    "                                               ('rf', rf_model)],                                                 \n",
    "                                   voting='hard')\n",
    "\n",
    "ensemble_model.fit(X_train_tfidf, train_df['label'])\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred_ensemble = ensemble_model.predict(X_test_tfidf)\n",
    "accuracy_ensemble = accuracy_score(test_df['label'], y_pred_ensemble)\n",
    "print(\"Accuracy using ensemble of models:\", accuracy_ensemble)\n",
    "\n",
    "# Preprocess the text input\n",
    "inp_txt = \"I have a solution for this problem\"\n",
    "inp_txt = inp_txt.lower()\n",
    "inp_txt = ' '.join([word for word in inp_txt.split() if word not in stop_words])\n",
    "inp_txt = word_tokenize(inp_txt)\n",
    "inp_txt = [stemmer.stem(word) for word in inp_txt]\n",
    "inp_txt = ' '.join(inp_txt)\n",
    "\n",
    "# Extract features from the preprocessed text input\n",
    "X_inp = tfidf_vectorizer.transform([inp_txt])\n",
    "\n",
    "# Predict the label of the input text using the logistic regression model\n",
    "y_pred_inp = ensemble_model.predict(X_inp)[0]\n",
    "\n",
    "# Print the predicted label\n",
    "print(\"Predicted label for input text:\", y_pred_inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ec59d",
   "metadata": {},
   "source": [
    "## Attempting GloVe\n",
    "\n",
    "link: [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86b8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # Convert the pre-trained GloVe embeddings to Word2Vec format\n",
    "# glove_file = 'path_to_glove_file'\n",
    "# word2vec_output_file = 'path_to_word2vec_output_file'\n",
    "# glove2word2vec(glove_file, word2vec_output_file)\n",
    "\n",
    "# # Load the pre-trained GloVe Word2Vec model\n",
    "# glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file)\n",
    "\n",
    "# # Function to convert a sentence to its vector representation using GloVe\n",
    "# def sentence_to_vec(sentence):\n",
    "#     word_vectors = [glove_model[word] for word in sentence if word in glove_model]\n",
    "#     if not word_vectors:\n",
    "#         # Return zero vector if no words are present in the GloVe model\n",
    "#         return np.zeros(glove_model.vector_size)\n",
    "#     sentence_vector = np.mean(word_vectors, axis=0)\n",
    "#     return sentence_vector\n",
    "\n",
    "# # Convert training and testing data to GloVe vectors\n",
    "# X_train_glove = np.array([sentence_to_vec(sentence) for sentence in train_df['status_text']])\n",
    "# X_test_glove = np.array([sentence_to_vec(sentence) for sentence in test_df['status_text']])\n",
    "\n",
    "# # Build a Multinomial Naive Bayes model using GloVe features\n",
    "# mnb_glove_model = MultinomialNB()\n",
    "# mnb_glove_model.fit(X_train_glove, train_df['label'])\n",
    "\n",
    "# # Evaluate the model on the testing set using GloVe features\n",
    "# y_pred_glove = mnb_glove_model.predict(X_test_glove)\n",
    "# accuracy_glove = accuracy_score(test_df['label'], y_pred_glove)\n",
    "# print(\"Accuracy using GloVe features:\", accuracy_glove)\n",
    "\n",
    "# # Build a logistic regression model using GloVe features\n",
    "# logreg_glove_model = LogisticRegression()\n",
    "# logreg_glove_model.fit(X_train_glove, train_df['label'])\n",
    "\n",
    "# # Evaluate the model on the testing set using GloVe features\n",
    "# y_pred_logreg_glove = logreg_glove_model.predict(X_test_glove)\n",
    "# accuracy_logreg_glove = accuracy_score(test_df['label'], y_pred_logreg_glove)\n",
    "# print(\"Accuracy using logistic regression and GloVe features:\", accuracy_logreg_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8128fe5",
   "metadata": {},
   "source": [
    "## Attempting Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ce5f07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, LSTM\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.layers import Embedding, LSTM\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Convert the sentences to sequences of word indices\n",
    "# tokenizer = word2vec_model.wv.get_keras_embedding(train_embeddings=False).tokenizer\n",
    "# sequences_train = tokenizer.texts_to_sequences(train_df['status_text'])\n",
    "# sequences_test = tokenizer.texts_to_sequences(test_df['status_text'])\n",
    "\n",
    "# # Pad sequences to have the same length\n",
    "# max_sequence_length = 100\n",
    "# X_train = pad_sequences(sequences_train, maxlen=max_sequence_length)\n",
    "# X_test = pad_sequences(sequences_test, maxlen=max_sequence_length)\n",
    "\n",
    "# # Create the deep learning model\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, weights=[word2vec_model.wv.vectors], input_length=max_sequence_length, trainable=False))\n",
    "# model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, train_df['label'], batch_size=64, epochs=10, validation_split=0.2)\n",
    "\n",
    "# # Evaluate the model on the testing set\n",
    "# loss, accuracy = model.evaluate(X_test, test_df['label'])\n",
    "# print(\"Accuracy using deep learning and Word2Vec features:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
